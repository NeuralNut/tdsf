{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import sys\n",
    "import os\n",
    "root = os.path.normpath(os.getcwd())\n",
    "# root = '/home/mitch/PythonProjects/learn_sciml/tdsf-pinn'\n",
    "# root = '/home/mitch/PythonProjects/tdsf-pinn'\n",
    "sys.path.append(f'{root}/src')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data.load_tdsf_data import load_tdsf_data\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_X_Y_cnsts(df, dt, ds, trunc, cnsts):\n",
    "\n",
    "    # define shape of PINN I/O\n",
    "    tv = np.arange(1,trunc+1)*dt\n",
    "    tv = tv[::ds]\n",
    "\n",
    "    # stack w,h, and t vectors to make inputs, X\n",
    "    df['X'] = df.apply(lambda x: np.concatenate((x.YIELD*np.ones((tv.size,1)), \n",
    "                                                 x.DEPTH*np.ones((tv.size,1)), \n",
    "                                                 np.reshape(tv,(-1,1))), axis=1).astype(np.float32), \n",
    "                       axis=1)\n",
    "    \n",
    "    # truncate and decimate tdsf vectors to make targets, Y\n",
    "    df['Y'] = df.DATA.map(lambda x: np.reshape(x[:trunc][::ds], (-1,1)).astype(np.float32))\n",
    "\n",
    "    # \n",
    "    df['cnsts'] = df.apply(lambda x: (x[cnsts].values*np.ones((tv.size,1))).astype(np.float32),\n",
    "                      axis=1)\n",
    "    return df\n",
    "\n",
    "# Define all hyperparams and run params here\n",
    "algo = 'PINN'\n",
    "experiment = 'agu2'\n",
    "\n",
    "# Data params\n",
    "vers = '002.1'\n",
    "materials = ['Tuff-Rhyolite']\n",
    "df = load_tdsf_data(vers, root)\n",
    "df = df[df['material'].isin(materials)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "trunc = 1000\n",
    "ds = 10\n",
    "dt = 0.001\n",
    "\n",
    "# velocity model constants associated with the tdsf\n",
    "cnsts = ['$h_o$', '$R_o$', '$y_0$', '$P_{1o}$', '$P_{2o}$', '$n$', 'C', '$V_S$', '$\\\\rho$']\n",
    "\n",
    "# process inputs (wht) and targets (tdsf)\n",
    "df = make_X_Y_cnsts(df, dt, ds, trunc, cnsts)\n",
    "# df, df_test = train_test_split(df, test_size=.99) # random: test_size={.50, .99}\n",
    "\n",
    "ylds, dpths = df.YIELD.quantile(.5, interpolation='nearest'), df.DEPTH.quantile(.5, interpolation='nearest')\n",
    "Q1 = (df.YIELD >= ylds) & (df.DEPTH >= dpths)\n",
    "Q2 = (df.YIELD >= ylds) & (df.DEPTH <= dpths)\n",
    "Q3 = (df.YIELD <= ylds) & (df.DEPTH <= dpths)\n",
    "Q4 = (df.YIELD <= ylds) & (df.DEPTH >= dpths)\n",
    "\n",
    "\n",
    "df_test = df[~Q4]\n",
    "df = df[Q4]\n",
    "# print(f'Yield: {df.YIELD.unique()} Depth: {df.DEPTH.unique()}')\n",
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons, act, regularization_param, regularization_exp, retrain_seed):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = act\n",
    "        self.regularization_param = regularization_param\n",
    "        # Regularization exponent\n",
    "        self.regularization_exp = regularization_exp\n",
    "        # Random seed for weight initialization\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def regularization(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.regularization_exp)\n",
    "        return self.regularization_param * reg_loss\n",
    "\n",
    "\n",
    "\n",
    "def exact_solution(w,h,t,con):\n",
    "    ''' \n",
    "    derived analytical expressions for ds/dw and ds/dh\n",
    "    w: (b, 1, 1) - yields of the point source\n",
    "    h: (b, 1, 1) - vertical source-to-receiver distance\n",
    "    t: (b, 1, nt) - time points\n",
    "    \n",
    "    '''\n",
    "    batch_size, _, nt = t.shape\n",
    "\n",
    "    ho,Ro,go,P1,P2,n,pv,sv,rho = con\n",
    "\n",
    "    Rel = Ro*((ho/h)**(1/n))*(w**(1/3))\n",
    "    ga = go*Ro/Rel\n",
    "    mu = rho*(sv**2)\n",
    "    lam = rho*(pv**2)-2*mu    \n",
    "    wo = pv/Rel\n",
    "    bet = (lam+2*mu)/(4*mu)\n",
    "    alp = wo/(2*bet)\n",
    "    p = wo*(1/2/bet-1/4/bet**2)**(1/2)\n",
    "\n",
    "    def dF_(t):\n",
    "        return (Rel*pv**2)/(4*mu*bet*p)*(-alp*torch.exp(-alp*t)*torch.sin(p*t) + p*torch.exp(-alp*t)*torch.cos(p*t))\n",
    "\n",
    "    def dBdR_(t):\n",
    "        t1 = t*ga*torch.exp(-ga*t)/Rel*P1*(h/ho)\n",
    "        t2 = (t*ga*torch.exp(-ga*(t))+3*(1-torch.exp(-ga*(t))))/Rel*P2*((ho/h)**(1/3))*((Ro/Rel)**3)*(w**(0.87))\n",
    "        return t1-t2\n",
    "    \n",
    "    dF = dF_(t)\n",
    "    dBdR = dBdR_(t)\n",
    "    \n",
    "    # convolve\n",
    "    dsdR = torch.nn.functional.conv1d(dF.view(1, batch_size, dF.size(-1)), \n",
    "                                    torch.flip(dBdR,dims=[-1]).view(batch_size, -1, dBdR.size(-1)), \n",
    "                                    padding='same', bias=None, groups=batch_size).view(batch_size, -1, nt)\n",
    "\n",
    "    dRdw = 1/3*Ro*((ho/h)**(1/n))*(w**(-2/3))\n",
    "    dRdh = -1/n*((ho/h)**(1/n))*(1/h)*(w**(1/3))\n",
    "    dsdw = dsdR*dRdw\n",
    "    dsdh = dsdR*dRdh\n",
    "    \n",
    "    return dF, dBdR, dsdR, dRdw, dRdh, dsdw, dsdh\n",
    "\n",
    "## PDE as loss function. Thus would use the network which we call as u_theta\n",
    "def f_partial(data, net, con, y):\n",
    "    '''\n",
    "    data: (b, nt, 3) unnormalized (..., [w,h,t]) vectors\n",
    "    '''\n",
    "\n",
    "    wvec = Variable(data[:,:,0], requires_grad=True) # (b, nt)\n",
    "    hvec = Variable(data[:,:,1], requires_grad=True) # (b, nt)\n",
    "    tvec = Variable(data[:,:,2], requires_grad=True) # (b, nt)\n",
    "\n",
    "    x = (wvec*hvec*tvec).reshape(-1, tvec.shape[-1], 1)\n",
    "\n",
    "    s = net((x - x.mean())/(x.std()))\n",
    "\n",
    "    \n",
    "    # remove normalization\n",
    "    s = s*(y.max() - y.min()) + y.min()\n",
    "    \n",
    "    s_w = torch.autograd.grad(s.mean(), wvec, create_graph=True)[0]\n",
    "    s_h = torch.autograd.grad(s.mean(), hvec, create_graph=True)[0]\n",
    "\n",
    "    \n",
    "    dF, dBdR, dsdR, dRdw, dRdh, dsdw, dsdh = exact_solution(wvec.mean(axis=-1).reshape(-1,1,1),\n",
    "                                                            hvec.mean(axis=-1).reshape(-1,1,1),\n",
    "                                                            tvec.reshape(data.shape[0],1,-1),\n",
    "                                                            con)\n",
    "    \n",
    "    \n",
    "    return s, s_w, dsdw, s_h, dsdh\n",
    "\n",
    "# Training params\n",
    "lr = 1.E-3\n",
    "ntz = 100\n",
    "cut = 10\n",
    "nt = ntz-cut\n",
    "\n",
    "# NN params\n",
    "input_size = nt\n",
    "output_size = input_size\n",
    "\n",
    "# Model and training params\n",
    "lam_d = 1. # data loss penalty\n",
    "\n",
    "# lam_sh = 1.e-9 # Physics loss penalty for mse_sh\n",
    "# lam_sw = 1.e-18 # Physics loss penalty for mse_sw\n",
    "# scale = 0.00001 # noise scale\n",
    "\n",
    "n_iter = 1000\n",
    "n_trials = 3\n",
    "\n",
    "# shuffle and sample by time points\n",
    "X, Y = np.vstack(df.X)[cut:ntz], np.vstack(df.Y)[cut:ntz]\n",
    "X_test, Y_test = np.vstack(df_test.X)[cut:ntz], np.vstack(df_test.Y)[cut:ntz]\n",
    "\n",
    "numIn = 1\n",
    "numOut = Y.shape[-1]\n",
    "\n",
    "X = torch.from_numpy(X.reshape(-1, nt, 3))\n",
    "Y = torch.from_numpy(Y.reshape(-1, nt, 1))\n",
    "\n",
    "X_test = torch.from_numpy(X_test.reshape(-1, nt, 3))\n",
    "Y_test = torch.from_numpy(Y_test.reshape(-1, nt, 1))\n",
    "\n",
    "con = [122., # ho\n",
    "       202., # Ro\n",
    "       26., # go\n",
    "       3.6*1e6, # P1\n",
    "       5.0*1e6, # P2\n",
    "       2.4, # n\n",
    "       3500, # pv\n",
    "       2021, # sv\n",
    "       2000 # rho\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "# loss, loss_d, loss_b, loss_sw, loss_sh, s_w, dsdw, s_h, dsdh = out\n",
    "# phy_loss_tr = loss_sw + loss_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import logging \n",
    "\n",
    "def train(trial):\n",
    "    # Setup model and optimizer.\n",
    "    model = NeuralNet(input_dimension=1, \n",
    "                    output_dimension=1, \n",
    "                    n_hidden_layers=6, \n",
    "                    neurons=96,\n",
    "                    act = torch.nn.GELU('tanh'),\n",
    "                    regularization_param=0., \n",
    "                    regularization_exp=2,\n",
    "                    retrain_seed=128).to(device)\n",
    "\n",
    "    # Select Optimizer and Loss Criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', .9999999, 100, cooldown=25)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    data, target = Variable(X, requires_grad=True).to(device), Variable(Y, requires_grad=True).to(device)\n",
    "    \n",
    "    lam_sw = trial.suggest_float(\"lam_sw\", 1.e-18, 1.e-10, log=True)\n",
    "    lam_sh = trial.suggest_float(\"lam_sh\", 1.e-9, 1.e-6, log=True)\n",
    "    scale = trial.suggest_float(\"scale\", 1.e-5, 1.e-2, log=True)\n",
    "    lam_d = trial.suggest_float(\"lam_d\", 1.e-2, 1.e2, log=True)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ln1, = ax.plot(Y[0].squeeze(), label='True')\n",
    "    ln2, = ax.plot(Y[0].squeeze(), label='Pred')\n",
    "    ln3, = ax.plot(Y[0].squeeze(), label='Noisy')\n",
    "    plt.legend()\n",
    "    plt.title(f'lam_d: {lam_d:.1e} lam_sw: {lam_sw:.1e} lam_sh: {lam_sh:.1e} scale: {scale:.1e}')\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        noise = scale*torch.randn_like(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, s_w, dsdw, s_h, dsdh = f_partial(data, model, con, target)\n",
    "        \n",
    "        #data loss\n",
    "        mse_d = criterion(output,target+noise) \n",
    "        loss_d = lam_d*mse_d\n",
    "\n",
    "        # boundary loss\n",
    "        t0 = torch.tensor(0.).to(device)\n",
    "        tl = torch.tensor(0.).to(device)\n",
    "        loss_b = criterion(output[:,0,:], t0) +  criterion(output[:,-1,:], tl)\n",
    "        \n",
    "        # physiscs loss\n",
    "        mse_sw = criterion(s_w.flatten().squeeze(), dsdw.flatten().squeeze())\n",
    "        mse_sh = criterion(s_h.flatten().squeeze(), dsdh.flatten().squeeze())\n",
    "        \n",
    "        loss_sw = lam_sw*mse_sw\n",
    "        loss_sh = lam_sh*mse_sh\n",
    "\n",
    "        loss = loss_d + loss_b + loss_sw + loss_sh\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        trial.report(mse_d.mean().item(), step=i)\n",
    "\n",
    "        print(f'Iteration: {i} Loss: {loss.item():.3e} da-{loss_d:.3e} bo-{loss_b:.3e} sw-{loss_sw:.3e} sh-{loss_sh:.3e} lr: {scheduler._last_lr[0]:.3e}', end='\\r')\n",
    "        if i == n_iter-1:\n",
    "            ln1.set_ydata(Y[0].squeeze())\n",
    "            ln2.set_ydata(output[0].squeeze().detach().cpu().numpy())\n",
    "            ln3.set_ydata(Y[0].squeeze()+noise[0].detach().cpu().squeeze())\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(pl.gcf())\n",
    "    \n",
    "    fig.savefig(f'{results_path}/{trial.number}.png')\n",
    "    fig.clear()\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(model, f'{results_path}/{trial.number}.pt')\n",
    "    \n",
    "    model.eval()\n",
    "    data, target = Variable(X_test, requires_grad=True).to(device), Variable(Y_test, requires_grad=True).to(device)\n",
    "    output, s_w, dsdw, s_h, dsdh = f_partial(data, model, con, target)\n",
    "        \n",
    "    #data loss\n",
    "    mse_d = criterion(output,target) \n",
    "    \n",
    "    return mse_d.mean().item()\n",
    "\n",
    "import pathlib\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"tdsf-study\"  # Unique identifier of the study.\n",
    "\n",
    "# Save results path\n",
    "results_path = f'{root}/experiments/{study_name}'\n",
    "pathlib.Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "storage_name = \"sqlite:///{}.db\".format(results_path)\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, sampler=RandomSampler(), load_if_exists=True)\n",
    "\n",
    "study.optimize(train, n_trials=n_trials)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_timeline\n",
    "\n",
    "# root = '/home/mitch/PythonProjects/learn_sciml/tdsf-pinn'\n",
    "root = os.path.normpath(os.getcwd())\n",
    "study_name = \"tdsf-study\" #\"tdsf-with-lam_d-study\" #    # Unique identifier of the study.\n",
    "results_path = f'{root}/experiments/{study_name}'\n",
    "storage_name = \"sqlite:///{}.db\".format(results_path)\n",
    "study = optuna.load_study(study_name, storage_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeline(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_slice(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
