{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 29 15:39:51 2023\n",
    "\n",
    "This script trains PINN model for TDSF supplied by AFTAC-CRADA\n",
    "Inputs: yield (w), depth (h), time (t)\n",
    "Output: displacement (S)\n",
    "\n",
    "@author: shhong\n",
    "\"\"\"\n",
    "\n",
    "#%% Load Moduels\n",
    "\n",
    "import os\n",
    "root = os.path.abspath('')\n",
    "# root = '/home/ubuntu/tdsf'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "study = 'noise_study_ood_domain_no_phy' #noise_study_ood_phy_domain_sampled\n",
    "\n",
    "#%% Load Data\n",
    "\n",
    "# path = 'E:\\Hong\\Research Projects\\PINN\\Data'\n",
    "path = f'{root}/data'\n",
    "\n",
    "filename = 'tdsf_002_train.pickle'\n",
    "df_train_all = pd.read_pickle(path + '/' + filename)\n",
    "\n",
    "filename = 'tdsf_002_val.pickle'\n",
    "df_test_all = pd.read_pickle(path + '/' + filename)\n",
    "\n",
    "df_data_all = pd.concat([df_train_all, df_test_all], axis=0)\n",
    "\n",
    "\n",
    "#%% Select Material\n",
    "\n",
    "# [Tuff-Rhyolite, Granite, Shale, Salt, Wet-Granite, Wet-Tuff]\n",
    "\n",
    "name_prefix = '_FAR.'\n",
    "name_material = 'Tuff-Rhyolite'\n",
    "\n",
    "df_data = df_data_all[df_data_all['material'] == name_prefix + name_material]\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "\n",
    "# round YIELD and DEPTH to 2 decimal places\n",
    "df_data.YIELD = df_data.YIELD.round(2)\n",
    "df_data.DEPTH = df_data.DEPTH.round(2)\n",
    "\n",
    "# splits df into train and test by yield and depth quantiles\n",
    "qnts = [.1, .9]\n",
    "ylds, dpths = df_data.YIELD.quantile(qnts, interpolation='nearest'), df_data.DEPTH.quantile(qnts, interpolation='nearest')\n",
    "train_qnt = (df_data.YIELD >= ylds.min()) & (df_data.YIELD <= ylds.max()) & (df_data.DEPTH >= dpths.min()) & (df_data.DEPTH <= dpths.max())\n",
    "\n",
    "# split data into train and test by domain (inner and outer)\n",
    "df_train = df_data[train_qnt].reset_index(drop=True)\n",
    "df_test = df_data[~train_qnt].reset_index(drop=True)\n",
    "\n",
    "# split train into train and validation, randomly selected from the inner domain\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=0)\n",
    "df_train, df_val = df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# plot train and test yields and depths\n",
    "s = 30\n",
    "plt.figure()\n",
    "plt.scatter(df_train.YIELD, df_train.DEPTH, label='Training Set', color='red', alpha=0.2, s=s)\n",
    "plt.scatter(df_val.YIELD, df_val.DEPTH, label='Validation Set', color='black', alpha=0.2, s=s)\n",
    "plt.scatter(df_test.YIELD, df_test.DEPTH, label='Test Set', color='blue', alpha=0.2, s=s)\n",
    "plt.xlabel('Yield')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.title('Data Domains')\n",
    "plt.savefig(f'{root}/results/{study}/domains.png')\n",
    "# plt.close('all')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Data Arrangement\n",
    "\n",
    "trunc = 1000\n",
    "ds = 10\n",
    "dt = 0.001\n",
    "\n",
    "tv = np.arange(1,trunc+1)*dt\n",
    "tv = tv[::ds]\n",
    "\n",
    "# Define Train Set\n",
    "tdsf = df_train.DATA\n",
    "wvec = df_train.YIELD\n",
    "hvec = df_train.DEPTH\n",
    "t_tdsf = tdsf[0][:trunc]\n",
    "Xtrain = np.concatenate((wvec[0]*np.ones((tv.size,1)), hvec[0]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)\n",
    "Ytrain = np.reshape(t_tdsf[::ds], (-1,1))\n",
    "for ndx in range(1, df_train.shape[0]):\n",
    "    t_tdsf = tdsf[ndx][:trunc]\n",
    "    Xtemp = np.concatenate((wvec[ndx]*np.ones((tv.size,1)), hvec[ndx]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)        \n",
    "    Xtrain = np.concatenate((Xtrain, Xtemp), axis=0)\n",
    "    Ytrain = np.concatenate((Ytrain, np.reshape(t_tdsf[::ds], (-1,1))), axis=0)\n",
    "Xtrain = Xtrain.astype(np.float32)\n",
    "Ytrain = Ytrain.astype(np.float32)\n",
    "\n",
    "# Define Val Set\n",
    "tdsf = df_val.DATA\n",
    "wvec = df_val.YIELD\n",
    "hvec = df_val.DEPTH\n",
    "t_tdsf = tdsf[0][:trunc]\n",
    "Xval = np.concatenate((wvec[0]*np.ones((tv.size,1)), hvec[0]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)\n",
    "Yval = np.reshape(t_tdsf[::ds], (-1,1))\n",
    "for ndx in range(1, df_val.shape[0]):\n",
    "    t_tdsf = tdsf[ndx][:trunc]\n",
    "    Xtemp = np.concatenate((wvec[ndx]*np.ones((tv.size,1)), hvec[ndx]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)        \n",
    "    Xval = np.concatenate((Xval, Xtemp), axis=0)\n",
    "    Yval = np.concatenate((Yval, np.reshape(t_tdsf[::ds], (-1,1))), axis=0)\n",
    "Xval = Xval.astype(np.float32)\n",
    "Yval = Yval.astype(np.float32)\n",
    "\n",
    "# Define Test Set\n",
    "tdsf = df_test.DATA\n",
    "wvec = df_test.YIELD\n",
    "hvec = df_test.DEPTH\n",
    "t_tdsf = tdsf[0][:trunc]\n",
    "Xtest = np.concatenate((wvec[0]*np.ones((tv.size,1)), hvec[0]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)\n",
    "Ytest = np.reshape(t_tdsf[::ds], (-1,1))\n",
    "for ndx in range(1, df_test.shape[0]):\n",
    "    t_tdsf = tdsf[ndx][:trunc]\n",
    "    Xtemp = np.concatenate((wvec[ndx]*np.ones((tv.size,1)), hvec[ndx]*np.ones((tv.size,1)), np.reshape(tv,(-1,1))), axis=1)        \n",
    "    Xtest = np.concatenate((Xtest, Xtemp), axis=0)\n",
    "    Ytest = np.concatenate((Ytest, np.reshape(t_tdsf[::ds], (-1,1))), axis=0)\n",
    "Xtest = Xtest.astype(np.float32)\n",
    "Ytest = Ytest.astype(np.float32)\n",
    "\n",
    "\n",
    "#%% Preprocessing\n",
    "\n",
    "# Min-Max Normalization\n",
    "Xscaler = MinMaxScaler()\n",
    "Xscaler.fit(np.vstack([Xtrain, Xval]))\n",
    "MaxX = Xscaler.data_max_\n",
    "MinX = Xscaler.data_min_\n",
    "\n",
    "Yscaler = MinMaxScaler()\n",
    "Yscaler.fit(np.vstack([Ytrain, Yval]))\n",
    "MaxY = Yscaler.data_max_\n",
    "MinY = Yscaler.data_min_\n",
    "\n",
    "Ztrain = Yscaler.transform(Ytrain)\n",
    "Zval = Yscaler.transform(Yval)\n",
    "Ztest = Yscaler.transform(Ytest)\n",
    "\n",
    "# Shuffle Data\n",
    "numdata = Xtrain.shape[0]\n",
    "Ztrain_base = Ztrain.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c77f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample w,h,t between their min and max values\n",
    "n = 4096\n",
    "\n",
    "w = np.random.uniform(MinX[0], MaxX[0], n)\n",
    "h = np.random.uniform(MinX[1], MaxX[1], n)\n",
    "t = np.random.uniform(MinX[2], MaxX[2], n)\n",
    "Xsample = np.vstack([w, h, t]).T\n",
    "\n",
    "\n",
    "\n",
    "# plot h vs w for the sampled data\n",
    "plt.figure()\n",
    "plt.scatter(w, h, s=10)\n",
    "plt.xlabel('Yield')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('Domain Sampling Example ')\n",
    "plt.savefig(f'{root}/results/{study}/sampled_data.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e4dd6-54f7-462f-822e-3c85c064c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = [0.00, 0.01, 0.1, 0.2, 0.5]\n",
    "for sigma in noise:\n",
    "\n",
    "    path = f'{root}/results/{study}/{sigma}'\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "    else:\n",
    "        print(f'{root}/results/{study}/{sigma} exists, skipping')\n",
    "        continue\n",
    "\n",
    "    # Gaussian Noise\n",
    "    # sigma = 0.00\n",
    "    mu = 0.00\n",
    "    np.random.seed(1)\n",
    "    Ztrain = (Ztrain_base + sigma*np.random.randn(numdata,1) + mu).astype(np.float32)\n",
    "\n",
    "    numIn = Xtrain.shape[1]\n",
    "    numOut = Ztrain.shape[1]\n",
    "\n",
    "\n",
    "    #%% Define Classes\n",
    "\n",
    "    class ANN_Dataset(Dataset):    \n",
    "\n",
    "        # Initialize your data\n",
    "        def __init__(self, x, y):        \n",
    "            self.len = x.shape[0]\n",
    "            self.x_data = torch.from_numpy(x)\n",
    "            self.y_data = torch.from_numpy(y)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.x_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "\n",
    "    # Fully Connected NN\n",
    "    class FCNN_Net(nn.Module):\n",
    "\n",
    "        def __init__(self, numIn, numOut):\n",
    "            super(FCNN_Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(numIn, 128)\n",
    "            self.fc2 = nn.Linear(128, 6)\n",
    "            self.fc3 = nn.Linear(6, numOut)\n",
    "            \n",
    "        def forward(self, w, h, t):        \n",
    "            inputs = torch.cat([w,h,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns        \n",
    "            # in_size = x.size(0)\n",
    "            # inputs = x.view(in_size, -1)\n",
    "            layer1_out = torch.tanh(self.fc1(inputs))\n",
    "            layer2_out = torch.tanh(self.fc2(layer1_out))\n",
    "            output = (self.fc3(layer2_out))\n",
    "            return output\n",
    "\n",
    "\n",
    "    #%% Defined Functions\n",
    "\n",
    "\n",
    "    ## PDE as loss function. Thus would use the network which we call as u_theta\n",
    "    def f_partial(data, net, MaxX, MinX, MaxY, MinY):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        xmax = torch.from_numpy(MaxX).to(device)\n",
    "        xmin = torch.from_numpy(MinX).to(device)\n",
    "        ymax = torch.from_numpy(MaxY).to(device)\n",
    "        ymin = torch.from_numpy(MinY).to(device)\n",
    "        \n",
    "        wvec = Variable(torch.reshape(data[:,0], (-1,1)), requires_grad=True).to(device)\n",
    "        hvec = Variable(torch.reshape(data[:,1], (-1,1)), requires_grad=True).to(device)\n",
    "        tvec = Variable(torch.reshape(data[:,2], (-1,1)), requires_grad=True).to(device)\n",
    "            \n",
    "        # applying normalized input\n",
    "        # x = torch.cat([(wvec-xmin[0])/(xmax[0]-xmin[0]),(hvec-xmin[1])/(xmax[1]-xmin[1]),(tvec-xmin[2])/(xmax[2]-xmin[2])],axis=1)    \n",
    "        s = net((wvec-xmin[0])/(xmax[0]-xmin[0]), (hvec-xmin[1])/(xmax[1]-xmin[1]), (tvec-xmin[2])/(xmax[2]-xmin[2])) # the dependent variable s is given by the network based on independent variables w, h, t\n",
    "\n",
    "        # remove normalization\n",
    "        s = s*(ymax-ymin) + ymin\n",
    "        \n",
    "        s_w = torch.autograd.grad(s.sum(), wvec, create_graph=True)[0]\n",
    "        s_h = torch.autograd.grad(s.sum(), hvec, create_graph=True)[0] \n",
    "            \n",
    "        ww = torch.reshape(data[:,0], (-1,1))\n",
    "        hh = torch.reshape(data[:,1], (-1,1))\n",
    "        tt = torch.reshape(data[:,2], (-1,1))\n",
    "        \n",
    "        # define parameters\n",
    "        ho = 122\n",
    "        Ro = 202\n",
    "        go = 26\n",
    "        P1 = 3.6*1e6\n",
    "        P2 = 5.0*1e6\n",
    "        n = 2.4\n",
    "        pv = 3500\n",
    "        sv = 2021\n",
    "        rho = 2000\n",
    "        \n",
    "        dsdR = torch.zeros(ww.shape).to(device)\n",
    "\n",
    "        Rel = Ro*((ho/hh)**(1/n))*(ww**(1/3))\n",
    "        ga = go*Ro/Rel\n",
    "\n",
    "        mu = rho*(sv**2)\n",
    "        lam = rho*(pv**2)-2*mu    \n",
    "        wo = pv/Rel\n",
    "        bet = (lam+2*mu)/(4*mu)\n",
    "        alp = wo/(2*bet)\n",
    "        p = wo*(1/2/bet-1/4/bet**2)**(1/2)\n",
    "\n",
    "        def dF_(tau):\n",
    "            return (Rel*pv**2)/(4*mu*bet*p)*(-alp*torch.exp(-alp*tau)*torch.sin(p*tau) + p*torch.exp(-alp*tau)*torch.cos(p*tau))\n",
    "\n",
    "        def dBdR_(tau):\n",
    "            return (tt-tau)*ga*torch.exp(-ga*(tt-tau))/Rel*P1*(hh/ho) - ((tt-tau)*ga*torch.exp(-ga*(tt-tau))+3*(1-torch.exp(-ga*(tt-tau))))/Rel*P2*((ho/hh)**(1/3))*((Ro/Rel)**3)*(ww**(0.87))\n",
    "\n",
    "        dtau = 0.001\n",
    "        tau = torch.tensor(np.arange(0,1,dtau)).to(device).float()\n",
    "        \n",
    "        dF = dF_(tau)\n",
    "        dBdR = dBdR_(tau)\n",
    "        \n",
    "        temp = dF*dBdR*dtau\n",
    "        mask = (tt - tau) < 0\n",
    "        temp[mask] = 0\n",
    "        dsdR += temp.sum(axis=-1).unsqueeze(-1)\n",
    "\n",
    "        dRdw = 1/3*Ro*((ho/hh)**(1/n))*(ww**(-2/3))\n",
    "        dRdh = -1/n*((ho/hh)**(1/n))*(1/hh)*(ww**(1/3))\n",
    "        dsdw = dsdR*dRdw\n",
    "        dsdh = dsdR*dRdh\n",
    "            \n",
    "        return s_w, dsdw, s_h, dsdh\n",
    "\n",
    "\n",
    "    def train(model, loader, optimizer, MaxX, MinX, MaxY, MinY, lam1, lam2):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        xmax = torch.from_numpy(MaxX).to(device)\n",
    "        xmin = torch.from_numpy(MinX).to(device)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "\n",
    "            \n",
    "            # get a batch of unnormlized data\n",
    "            data, target = Variable(data, requires_grad=True).to(device), Variable(target, requires_grad=True).to(device)\n",
    "\n",
    "            # create Y-D quantile mask for data loss\n",
    "            mask = (data[:,0] >= ylds.min()) & (data[:,0] <= ylds.max()) & (data[:,1] >= dpths.min()) & (data[:,1] <= dpths.max())\n",
    "\n",
    "            # randomly sample w,h,t between their min and max values\n",
    "            w = np.random.uniform(MinX[0], MaxX[0], batch_size)\n",
    "            h = np.random.uniform(MinX[1], MaxX[1], batch_size)\n",
    "            t = np.random.uniform(MinX[2], MaxX[2], batch_size)\n",
    "            Xsample = Variable(torch.from_numpy(np.vstack([w, h, t]).T), requires_grad=True).float().to(device)\n",
    "            \n",
    "            # normalization\n",
    "            dataN = (data-xmin)/(xmax-xmin)\n",
    "                    \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(torch.reshape(dataN[:,0], (-1,1)), torch.reshape(dataN[:,1], (-1,1)), torch.reshape(dataN[:,2], (-1,1)))\n",
    "                \n",
    "                # mask the output and target in the data loss calculation\n",
    "                mse_d = criterion(output[mask],target[mask])    \n",
    "                \n",
    "                # physics loss spans the entire domain\n",
    "                s_w, dsdw, s_h, dsdh = f_partial(Xsample, model, MaxX, MinX, MaxY, MinY)\n",
    "\n",
    "                mse_sw = criterion(s_w,dsdw)\n",
    "                mse_sh = criterion(s_h,dsdh)\n",
    "                \n",
    "                \n",
    "                loss = mse_d \n",
    "\n",
    "                loss.backward()\n",
    "                return loss + \n",
    "            \n",
    "            loss = optimizer.step(closure=closure)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        total_loss /= len(loader.dataset)\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "\n",
    "    def test(model, loader, MaxX, MinX, MaxY, MinY):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        \n",
    "        xmax = torch.from_numpy(MaxX).to(device)\n",
    "        xmin = torch.from_numpy(MinX).to(device)\n",
    "        \n",
    "        for data, target in loader:\n",
    "\n",
    "            data = data.requires_grad_(False).to(device)\n",
    "            target = target.requires_grad_(False).to(device)\n",
    "            \n",
    "            # normalization\n",
    "            dataN = (data-xmin)/(xmax-xmin)\n",
    "            \n",
    "            output = model(torch.reshape(dataN[:,0], (-1,1)), torch.reshape(dataN[:,1], (-1,1)), torch.reshape(dataN[:,2], (-1,1)))\n",
    "            \n",
    "            loss = criterion(output,target)\n",
    "            test_loss += loss.item()\n",
    "            # print(len(data))\n",
    "\n",
    "        test_loss /= len(loader.dataset)\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    #%% Hyperparameter Selection\n",
    "\n",
    "    max_epoch = 1000\n",
    "    max_fail = 8\n",
    "    batch_size = 4096\n",
    "\n",
    "\n",
    "    lamda1 = 1e-9 # PDE loss weight\n",
    "    lamda2 = 1.\n",
    "\n",
    "\n",
    "    train_dataset = ANN_Dataset(Xtrain, Ztrain)\n",
    "    val_dataset = ANN_Dataset(Xval, Zval)\n",
    "    test_dataset = ANN_Dataset(Xtest, Ztest)\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    #%% Training\n",
    "\n",
    "    # Define Network to Train\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = FCNN_Net(numIn, numOut).to(device)\n",
    "    # model.double()\n",
    "\n",
    "    # Select Optimizer and Loss Criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                            weight_decay=0, amsgrad=False)\n",
    "\n",
    "    fail_count = 0\n",
    "    min_cost = float('inf')\n",
    "\n",
    "    timevec = np.zeros((max_epoch, 1))\n",
    "    losses = {'train': [], 'val': [], 'test': []}\n",
    "    \n",
    "    for epoch in range(1, max_epoch):\n",
    "        \n",
    "        start = timer()    \n",
    "        RMSE_train_DP = np.sqrt(train(model, train_loader, optimizer, MaxX, MinX, MaxY, MinY, lamda1, lamda2))\n",
    "        RMSE_train_D = np.sqrt(test(model, train_loader, MaxX, MinX, MaxY, MinY))\n",
    "        RMSE_val = np.sqrt(test(model, val_loader, MaxX, MinX, MaxY, MinY))\n",
    "        RMSE_test = np.sqrt(test(model, test_loader, MaxX, MinX, MaxY, MinY))\n",
    "\n",
    "        losses['train'].append(RMSE_train_D)\n",
    "        losses['val'].append(RMSE_val)\n",
    "        losses['test'].append(RMSE_test)\n",
    "\n",
    "        print(f'Epoch: {epoch} Trn (D+P) {RMSE_train_DP:.8f} | Trn (D) {RMSE_train_D:.8f} | Val {RMSE_val:.8f} | Tst {RMSE_test:.8f}')\n",
    "            \n",
    "        if RMSE_val < min_cost:\n",
    "            min_cost = RMSE_val\n",
    "            fail_count = 0\n",
    "        elif fail_count >= max_fail:\n",
    "            print('Stopping... Maximum number of validation failures exceeded')\n",
    "            break\n",
    "        else:\n",
    "            fail_count += 1\n",
    "                \n",
    "        timevec[epoch] = timer() - start\n",
    "            \n",
    "    time_avg = timevec[:epoch].mean()\n",
    "    print('Average time taken for a single epoch is %f seconds.' % time_avg)\n",
    "\n",
    "    #%% Plot Results\n",
    "\n",
    "    plt.plot(losses['train'], 'b')\n",
    "    plt.plot(losses['val'], 'r')\n",
    "    plt.plot(losses['test'], 'g')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(('train', 'val', 'test'))\n",
    "    plt.savefig(path + '/losses.png')\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    # validation set\n",
    "    Xin = (Xval-MinX)/(MaxX-MinX)\n",
    "    Xt = torch.from_numpy(Xin)\n",
    "    Zhat = model(torch.reshape(Xt[:,0], (-1,1)), torch.reshape(Xt[:,1], (-1,1)), torch.reshape(Xt[:,2], (-1,1)))\n",
    "    Zhat = Zhat.detach().numpy()\n",
    "    Yhat_val = Yscaler.inverse_transform(Zhat)\n",
    "\n",
    "\n",
    "    # train set\n",
    "    Xin = (Xtrain-MinX)/(MaxX-MinX)\n",
    "    Xt = torch.from_numpy(Xin)\n",
    "    Zhat = model(torch.reshape(Xt[:,0], (-1,1)), torch.reshape(Xt[:,1], (-1,1)), torch.reshape(Xt[:,2], (-1,1)))\n",
    "    Zhat = Zhat.detach().numpy()\n",
    "    Yhat_train = Yscaler.inverse_transform(Zhat)\n",
    "\n",
    "    \n",
    "    line = np.arange(0, MaxY, 1e-4)\n",
    "    plt.plot(Yval[:,0], Yhat_val[:,0], 'bo')\n",
    "    plt.plot(line, line, 'k')\n",
    "    plt.xlabel('true')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.title('displacement')\n",
    "    plt.savefig(path + '/disp_true_v_pred_val.png')\n",
    "    # plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    # test set\n",
    "    Xin = (Xtest-MinX)/(MaxX-MinX)\n",
    "    Xt = torch.from_numpy(Xin)\n",
    "    Zhat = model(torch.reshape(Xt[:,0], (-1,1)), torch.reshape(Xt[:,1], (-1,1)), torch.reshape(Xt[:,2], (-1,1)))\n",
    "    Zhat = Zhat.detach().numpy()\n",
    "    Yhat_test = Yscaler.inverse_transform(Zhat)\n",
    "\n",
    "\n",
    "    line = np.arange(0, MaxY, 1e-4)\n",
    "    plt.plot(Ytest[:,0], Yhat_test[:,0], 'bo')\n",
    "    plt.plot(line, line, 'k')\n",
    "    plt.xlabel('true')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.title('displacement')\n",
    "    plt.savefig(path + '/disp_true_v_pred_test.png')\n",
    "    # plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(Xtest[:,2], Yhat_test[:,0], 'bo')\n",
    "    plt.plot(Xtest[:,2], Ytest[:,0], 'r')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('displacement')\n",
    "    plt.savefig(path + '/disp_err_v_time.png')\n",
    "    # plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%% Save Data and Model\n",
    "\n",
    "    # v1: lamda1 = 1e-9, lamda2 = 1 (noise 0) RMSE: 0.0001379581\n",
    "\n",
    "\n",
    "    modelname = 'SeismicWave_TDSF_forward_PGNN_v1.pth'\n",
    "    ANNmodel = {'model': FCNN_Net(numIn, numOut),\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(ANNmodel, path + '/' + modelname)\n",
    "\n",
    "\n",
    "    filename = f'{path}/SeismicWave_TDSF_forward_PGNN_v1_result.mat'\n",
    "    sio.savemat(filename, \n",
    "            {\n",
    "            'Yhat_train':Yhat_train, 'Ytrain':Ytrain, 'Xtrain':Xtrain,\n",
    "            'Yhat_test':Yhat_test, 'Ytest':Ytest, 'Xtest':Xtest, \n",
    "            'Yhat_val':Yhat_val,'Yval':Yval, 'Xval':Xval,\n",
    "            'Ymax':MaxY, 'Ymin':MinY, 'Xmax':MaxX, 'Xmin':MinX\n",
    "            })\n",
    "\n",
    "    #%% Result Comparison\n",
    "\n",
    "    modelname = 'SeismicWave_TDSF_forward_PGNN_v1.pth'\n",
    "    model_data1 = torch.load(path + '/' + modelname)\n",
    "\n",
    "    model1 = model_data1['model']\n",
    "    model1.load_state_dict(model_data1['state_dict'])\n",
    "    model1.eval()\n",
    "\n",
    "\n",
    "    TI = 2\n",
    "    Ind1 = int((TI-1)*100)\n",
    "    Ind2 = int(TI*100)\n",
    "    tv = Xtest[Ind1:Ind2,2]\n",
    "    Yreal = np.squeeze(Ytest[Ind1:Ind2])\n",
    "    w,t = Xtest[Ind1,0], Xtest[Ind1,1]\n",
    "\n",
    "    Xin = (Xtest-MinX)/(MaxX-MinX)\n",
    "\n",
    "    Ztemp1 = model1(torch.from_numpy(np.reshape(Xin[Ind1:Ind2,0],(-1,1))),\n",
    "                    torch.from_numpy(np.reshape(Xin[Ind1:Ind2,1],(-1,1))),\n",
    "                    torch.from_numpy(np.reshape(Xin[Ind1:Ind2,2],(-1,1)))).detach().numpy()\n",
    "    Ytemp1 = np.squeeze(Yscaler.inverse_transform(Ztemp1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # Plot desired concentration    \n",
    "    plt.plot(tv, Yreal, 'k')\n",
    "    plt.plot(tv, Ytemp1, 'b')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('displacement')\n",
    "    plt.legend(('Truth',f'{sigma}% Noise'))\n",
    "    plt.title(f'w = {w}, h = {t}')\n",
    "    plt.savefig(path + '/t_v_d.png')\n",
    "    # plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot desired concentration    \n",
    "    plt.plot(tv, np.abs(Ytemp1-Yreal), 'b')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('displacement error')\n",
    "    plt.legend((f'{sigma}% Noise'))\n",
    "    plt.savefig(path + '/t_v_d_err.png')\n",
    "    plt.close('all')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%% Correlation Check\n",
    "\n",
    "    from scipy import signal\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    corr1 = signal.correlate(Ytemp1, Yreal, mode='full', method='auto')\n",
    "    corr1 /= np.max(corr1)\n",
    "    lags1 = signal.correlation_lags(len(Ytemp1), len(Yreal))\n",
    "\n",
    "    plt.plot(lags1, corr1, 'b')\n",
    "    plt.xlabel('lags')\n",
    "    plt.ylabel('cross-correlation')\n",
    "    plt.legend((f'{sigma}% Noise'))\n",
    "    plt.savefig(path + '/cross_corr.png')\n",
    "    plt.close('all')\n",
    "    # plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea89af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ea942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
